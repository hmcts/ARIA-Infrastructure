variables:
  - group: DatabricksSecrets
  - name: shouldBuildWheel
    value: 'true'
  - name: mainRepositoryName
    value: '$(Build.SourcesDirectory)/01-Sandbox'
  - name: pythonVersion
    value: '3.9'
  - name: subscriptionEndPoint
    value: 'test_python_installation' 
  - name: wheelFile
    value: "$(Pipeline.Workspace)/ARIAFUNCITONS-0.0.1-py3-none-any.whl"
  - name: wheelName
    value: 'ARIAFUNCITONS-0.0.1-py3-none-any.whl'
  - name: workspacePath
    value: '/Workspace/Shared/wheels'

stages:
- stage: Build
  displayName: 'Build Wheel Package'
  jobs:
  - job: BuildWheelPackage
    displayName: 'Build Python Wheel'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - checkout: self
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'
        addToPath: true
        architecture: 'x64'

    - script: |
        python --version
        pip install --upgrade pip
        pip install build
        python -m build --wheel --outdir $(mainRepositoryName)/Databricks/SharedFunctionsLib/dist \
          $(mainRepositoryName)/Databricks/SharedFunctionsLib

        ls -la $(mainRepositoryName)/Databricks/SharedFunctionsLib/dist
      displayName: 'Install Python dependencies & build Python wheel'

    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/01-Sandbox/Databricks/SharedFunctionsLib/dist'
        ArtifactName: 'ariafunction'
        publishLocation: 'Container'
      displayName: 'Publish Wheel Artifact'

- stage: Deploy
  displayName: 'Deploy to Databricks'
  dependsOn: Build
  jobs:
  - job: DeployToDatabricks
    displayName: 'Deploy to Databricks'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'ariafunction'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download wheel Artifact'
    
    - script: |
        echo "Contents of Artifact Staging Directory:"
        find '$(Pipeline.Workspace)' -type f | sort
        echo "-----"
        find '$(Pipeline.Workspace)' -name "*.whl"
        echo "-----"
        ls -la '$(Pipeline.Workspace)'
      displayName: 'List contents of Artifact Staging Directory'

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'
        addToPath: true
        architecture: 'x64'
    
    - script: |
        pip install databricks-cli --upgrade
      displayName: 'Install Databricks CLI'
    
    - script: |
        echo "[DEFAULT]" > ~/.databrickscfg
        echo "host = $(databricksInstance)" >> ~/.databrickscfg
        echo "token = $(databricksPAT)" >> ~/.databrickscfg
      displayName: 'Configure Databricks CLI'

    - script: |
       set -e
        WHEEL_FILE=$(find '$(Pipeline.Workspace)' -name "*.whl" | head -n 1)
        if [ -n "$WHEEL_FILE" ]; then
          echo "Found wheel file: $WHEEL_FILE"
          FILENAME=$(basename "$WHEEL_FILE")
          
          # Upload to temporary DBFS location
          databricks fs cp --overwrite "$WHEEL_FILE" "dbfs:/temp_wheels/$FILENAME"
          
          # Create a temporary notebook for installation
          echo '{
            "content": "%pip install /dbfs/temp_wheels/'$FILENAME'",
            "path": "/Shared/Temporary_Wheel_Install",
            "language": "PYTHON",
            "overwrite": true
          }' > notebook.json
          
          databricks workspace import --format SOURCE --language PYTHON --file notebook.json
          
          # Run the notebook on the existing cluster
          JOB_RUN=$(databricks jobs run-now --json '{
            "notebook_task": {
              "notebook_path": "/Shared/Temporary_Wheel_Install"
            },
            "existing_cluster_id": "$(databricksClusterId)"
          }')
          
          RUN_ID=$(echo $JOB_RUN | jq -r '.run_id')
          echo "Installation job started with run_id: $RUN_ID"
          
          # Wait for job completion
          while true; do
            STATUS=$(databricks runs get --run-id $RUN_ID | jq -r '.state.life_cycle_state')
            echo "Job status: $STATUS"
            
            if [ "$STATUS" = "TERMINATED" ] || [ "$STATUS" = "SKIPPED" ] || [ "$STATUS" = "INTERNAL_ERROR" ]; then
              RESULT=$(databricks runs get --run-id $RUN_ID | jq -r '.state.result_state')
              if [ "$RESULT" = "SUCCESS" ]; then
                echo "Installation successful"
                exit 0
              else
                echo "Installation failed"
                databricks runs get-output --run-id $RUN_ID
                exit 1
               fi
            fi
            sleep 5
          done
        else
          echo "No wheel file found"
          exit 1
        fi
      displayName: 'Install Wheel via Notebook Job'
    - script: |
       WHEEL_FILENAME=$(basename $(find "$(Pipeline.Workspace)" -name "*.whl" | head -n 1))
       if [ -z "$WHEEL_FILENAME" ]; then
         echo "Error: No .whl file found in $(Pipeline.Workspace)"
         exit 1
       fi
       echo "Installing wheel: $WHEEL_FILENAME on cluster ID: $(databricksClusterId)"
       databricks libraries install --cluster-id $(databricksClusterId) --whl "workspace:$(workspacePath)/$WHEEL_FILENAME"
      displayName: 'Install Wheel on Databricks Cluster'

    - script: |
        # Restart cluster to ensure library is loaded
        databricks clusters restart --cluster-id "$(databricksClusterId)"
      displayName: 'Restart Databricks Cluster'