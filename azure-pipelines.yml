variables:
  - group: DatabricksSecrets
  - group: Secrets
  - name: shouldBuildWheel
    value: 'true'
  - name: mainRepositoryName
    value: '$(Build.SourcesDirectory)/01-Sandbox'
  - name: pythonVersion
    value: '3.9'
  - name: subscriptionEndPoint
    value: 'test_python_installation' 
  - name: wheelFile
    value: "$(Pipeline.Workspace)/ARIAFUNCITONS-0.0.1-py3-none-any.whl"
  - name: wheelName
    value: 'ARIAFUNCITONS-0.0.1-py3-none-any.whl'
  - name: workspacePath
    value: 'dbfs:/FileStore/shared_wheels/'
  - name: HTML_StorageAccount
    value: 'ingest02landingsbox'
  - name: Resource_Group
    value: 'ingest02-main-sbox'

stages:
- stage: Build
  displayName: 'Build Wheel Package'
  jobs:
  - job: BuildWheelPackage
    displayName: 'Build Python Wheel'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - checkout: self
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'
        addToPath: true
        architecture: 'x64'

    - script: |
        python --version
        pip install --upgrade pip
        pip install build
        python -m build --wheel --outdir $(mainRepositoryName)/Databricks/SharedFunctionsLib/dist \
          $(mainRepositoryName)/Databricks/SharedFunctionsLib

        ls -la $(mainRepositoryName)/Databricks/SharedFunctionsLib/dist
        echo "listing contents of mainRepositoryName:$(mainRepositoryName)"
        ls -la "$(mainRepositoryName)/"

      displayName: 'Install Python dependencies & build Python wheel'

    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/01-Sandbox/Databricks/SharedFunctionsLib/dist'
        ArtifactName: 'ariafunction'
        publishLocation: 'Container'
      displayName: 'Publish Wheel Artifact'
    
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/01-Sandbox/requirements.txt'
        ArtifactName: 'pythondependencies'
        publishLocation: 'Container'
      displayName: 'Publish Wheel Artifact'

- stage: Deployment
  displayName: 'Deployment'
  dependsOn: Build
  jobs:
  - job: DeployToDatabricks
    displayName: 'Deploy to Databricks'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'ariafunction'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download wheel Artifact'
    
    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'pythondependencies'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download Python Libraries Artifact'
    
    - script: |
        echo "Contents of Artifact Staging Directory:"
        find '$(Pipeline.Workspace)' -type f | sort
        echo "-----"
        find '$(Pipeline.Workspace)' -name "*.whl"
        echo "-----"
        ls -la '$(Pipeline.Workspace)'
      displayName: 'List contents of Artifact Staging Directory'

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'
        addToPath: true
        architecture: 'x64'
    
    - script: |
        pip install databricks-cli --upgrade
      displayName: 'Install Databricks CLI'
    
    - script: |
        echo "[DEFAULT]" > ~/.databrickscfg
        echo "host = $(databricksInstance)" >> ~/.databrickscfg
        echo "token = $(databricksPAT)" >> ~/.databrickscfg
      displayName: 'Configure Databricks CLI'

    - script: |
        set -e 

        WHEEL_FILE=$(find '$(Pipeline.Workspace)' -name "*.whl" | tail -n 1)

        if [ -n "$WHEEL_FILE" ]; then
          echo "Found wheel file: $WHEEL_FILE"

          FILENAME=$(basename "$WHEEL_FILE")
          echo "Extracted filename: $FILENAME"
  
          DEST_PATH="dbfs:/FileStore/shared_wheels/ARIAFUNCITONS-0.0.1-py3-none-any.whl"
          echo "Destination path: $DEST_PATH"

          echo "copying wheel file to DBFS..."
          echo "copying $WHEEL_FILE to $DEST_PATH"
          databricks fs cp "$WHEEL_FILE" "$DEST_PATH" --overwrite --debug

          echo "Listing files in $(workspacePath)/"

          if [ $? -eq 0 ]; then
            echo "Wheel file uploaded successfully to $(workspacePath)/"
            exit 0
          else
            echo "No wheel files found!"
            exit 1
          fi
        else
          echo "No wheel file found"
          exit 1
        fi
      displayName: 'Upload Wheel to Databricks DBFS'

    - script: |
       WHEEL_FILENAME=$(basename $(find "$(Pipeline.Workspace)" -name "*.whl" | head -n 1))
       if [ -z "$WHEEL_FILENAME" ]; then
         echo "Error: No .whl file found in $(Pipeline.Workspace)"
         exit 1
       fi
       echo "Installing wheel: $WHEEL_FILENAME on cluster ID: $(databricksClusterId)"
       databricks libraries install --cluster-id $(databricksClusterId) --whl "$(workspacePath)/$WHEEL_FILENAME"
      displayName: 'Install Wheel on Databricks Cluster'
    
    - script: |
       PYTHON_PACKAGE=$(find "$(Pipeline.Workspace)/pythondependencies" -name "*.txt" | head -n 1)
       if [ -z "$PYTHON_PACKAGE" ]; then
         echo "Error: No .txt file found in $(Pipeline.Workspace)/pythondependencies"
         exit 1
       fi
       echo "Uploading requirements.txt to DBFS..."
       databricks fs cp "$PYTHON_PACKAGE" dbfs:/FileStore/dependencies/requirements.txt --overwrite
       echo "Successfully uploaded "$PYTHON_PACKAGE" to dbfs:/FileStore/dependencies/requirements.txt"
      displayName: 'Upload Python (requirements.txt) packages to DBFS'
    
    - script: |
        echo "Installing Python packages on cluster ID: $(databricksClusterId)"
        databricks fs cp dbfs:/FileStore/dependencies/requirements.txt requirements.txt
        pip install -r requirements.txt
        echo "Succesfully installed Python packages!"
      displayName: 'Install Python (requirements.txt) packages'

    - script: |
       echo "Backup Installing Kafka and Eventhub"
       databricks libraries install --cluster-id $(databricksClusterId) --pypi-package azure-eventhub
       databricks libraries install --cluster-id $(databricksClusterId) --pypi-package confluent-kafka
      displayName: 'Install Kafka and Eventhub to Cluster'
      
  - job: DeployHTMLtoADLS
    displayName: 'Deploy HTML to ADLS'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
      - task: AzureCLI@2
        inputs:
          azureSubscription: 'DTS-DATAINGEST-SBOX'
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            az role assignment create \
            --assignee $(Client_ID) \
            --role "Storage Blob Data Contributor" \
            --scope /subscriptions/$(Subscription_ID)/resourceGroups/$(Resource_Group)/providers/Microsoft.Storage/storageAccounts/$(HTML_StorageAccount)
        displayName: 'Assigning Service Principal Blob Contributor'

      - task: AzureCLI@2
        inputs:
          azureSubscription: 'DTS-DATAINGEST-SBOX'
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            
            # Automatic yes prompt
            export AZURE_STORAGE_YES_PROMPT=true
            
            # Get the agent outbound IP
            AGENT_IP=$(curl -s https://api64.ipify.org)
            echo "Agent IP: $AGENT_IP"

            # Add agent IP to storage account network rules
            az storage account network-rule add \
            --resource-group $(Resource_Group) \
            --account-name $(HTML_StorageAccount) \
            --ip-address $AGENT_IP
        displayName: 'Add IP to Storage account network rules'
          
      - task: AzureCLI@2
        inputs:
          azureSubscription: 'DTS-DATAINGEST-SBOX'
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            storage_account_key=$(az storage account keys list \
            --resource-group $(Resource_Group) \
            --account-name $(HTML_StorageAccount) \
            --query '[0].value' -o tsv)

            echo "##vso[task.setvariable variable=storage_account_key]$storage_account_key"
        displayName: 'Retrieve $(HTML_StorageAccount) storage account key'

      - task: AzureCLI@2
        inputs:
          azureSubscription: 'DTS-DATAINGEST-SBOX'
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            az storage account update \
            --name $(HTML_StorageAccount) \
            --resource-group $(Resource_Group) \
            --bypass AzureServices
        displayName: 'Update $(HTML_StorageAccount) to bypass Firewall'
          
      - task: AzureCLI@2
        inputs:
          azureSubscription: 'DTS-DATAINGEST-SBOX'
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            az storage container create \
            --account-name $(HTML_StorageAccount) \
            --account-key $storage_account_key \
            --name html-template \
            --public-access off
        displayName: 'Create container (html-template) if does not exist'

      - task: AzureCLI@2
        inputs:
          azureSubscription: 'DTS-DATAINGEST-SBOX'
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            # Upload HTML templates to blob storage
            az storage blob upload-batch \
            --account-name $(HTML_StorageAccount) \
            --account-key $storage_account_key \
            --destination html-template \
            --source '$(Build.SourcesDirectory)/01-Sandbox/HTML_Templates' \
            --pattern '*.html' 
          
            echo "Upload completed successfully!"
        displayName: 'Upload HTML templates to ADLS'