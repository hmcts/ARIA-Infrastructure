variables:
  - group: DatabricksSecrets
  - group: Secrets
  - name: shouldBuildWheel
    value: 'true'
  - name: mainRepositoryName
    value: '$(Build.SourcesDirectory)/01-Sandbox'
  - name: pythonVersion
    value: '3.9'
  - name: subscriptionEndPoint
    value: 'test_python_installation' 
  - name: wheelFile
    value: "$(Pipeline.Workspace)/ARIAFUNCITONS-0.0.1-py3-none-any.whl"
  - name: wheelName
    value: 'ARIAFUNCITONS-0.0.1-py3-none-any.whl'
  - name: workspacePath
    value: 'dbfs:/FileStore/shared_wheels/'
  - name: HTML_StorageAccount
    value: 'ingest02landingsbox'
  - name: Resource_Group
    value: 'ingest02-main-sbox'

stages:
- stage: Build
  displayName: 'Build Wheel Package'
  jobs:
  - job: BuildWheelPackage
    displayName: 'Build Python Wheel'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - checkout: self
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'
        addToPath: true
        architecture: 'x64'

    - script: |
        python --version
        pip install --upgrade pip
        pip install build
        python -m build --wheel --outdir $(mainRepositoryName)/Databricks/SharedFunctionsLib/dist \
          $(mainRepositoryName)/Databricks/SharedFunctionsLib

        ls -la $(mainRepositoryName)/Databricks/SharedFunctionsLib/dist
        echo "listing contents of mainRepositoryName:$(mainRepositoryName)"
        ls -la "$(mainRepositoryName)/"

      displayName: 'Install Python dependencies & build Python wheel'

    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/01-Sandbox/Databricks/SharedFunctionsLib/dist'
        ArtifactName: 'ariafunction'
        publishLocation: 'Container'
      displayName: 'Publish Wheel Artifact'
    
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/01-Sandbox/requirements.txt'
        ArtifactName: 'pythondependencies'
        publishLocation: 'Container'
      displayName: 'Publish Wheel Artifact'

- stage: Deployment
  displayName: 'Deployment'
  dependsOn: Build
  jobs:
  - job: DeployToDatabricks
    displayName: 'Deploy to Databricks'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'ariafunction'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download wheel Artifact'
    
    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'pythondependencies'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download Python Libraries Artifact'
    
    - script: |
        echo "Contents of Artifact Staging Directory:"
        find '$(Pipeline.Workspace)' -type f | sort
        echo "-----"
        find '$(Pipeline.Workspace)' -name "*.whl"
        echo "-----"
        ls -la '$(Pipeline.Workspace)'
      displayName: 'List contents of Artifact Staging Directory'

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'
        addToPath: true
        architecture: 'x64'
    
    - script: |
        pip install databricks-cli --upgrade
      displayName: 'Install Databricks CLI'
    
    - script: |
        echo "[DEFAULT]" > ~/.databrickscfg
        echo "host = $(databricksInstance)" >> ~/.databrickscfg
        echo "token = $(databricksPAT)" >> ~/.databrickscfg
      displayName: 'Configure Databricks CLI'

    - script: |
        set -e 

        WHEEL_FILE=$(find '$(Pipeline.Workspace)' -name "*.whl" | tail -n 1)

        if [ -n "$WHEEL_FILE" ]; then
          echo "Found wheel file: $WHEEL_FILE"

          FILENAME=$(basename "$WHEEL_FILE")
          echo "Extracted filename: $FILENAME"
  
          DEST_PATH="dbfs:/FileStore/shared_wheels/ARIAFUNCITONS-0.0.1-py3-none-any.whl"
          echo "Destination path: $DEST_PATH"

          echo "copying wheel file to DBFS..."
          echo "copying $WHEEL_FILE to $DEST_PATH"
          databricks fs cp "$WHEEL_FILE" "$DEST_PATH" --overwrite --debug

          echo "Listing files in $(workspacePath)/"

          if [ $? -eq 0 ]; then
            echo "Wheel file uploaded successfully to $(workspacePath)/"
            exit 0
          else
            echo "No wheel files found!"
            exit 1
          fi
        else
          echo "No wheel file found"
          exit 1
        fi
      displayName: 'Upload Wheel to Databricks DBFS'

    - script: |
       WHEEL_FILENAME=$(basename $(find "$(Pipeline.Workspace)" -name "*.whl" | head -n 1))
       if [ -z "$WHEEL_FILENAME" ]; then
         echo "Error: No .whl file found in $(Pipeline.Workspace)"
         exit 1
       fi
       echo "Installing wheel: $WHEEL_FILENAME on cluster ID: $(databricksClusterId)"
       databricks libraries install --cluster-id $(databricksClusterId) --whl "$(workspacePath)/$WHEEL_FILENAME"
      displayName: 'Install Wheel on Databricks Cluster'
    
    - script: |
       PYTHON_PACKAGE=$(find "$(Pipeline.Workspace)/pythondependencies" -name "*.txt" | head -n 1)
       if [ -z "$PYTHON_PACKAGE" ]; then
         echo "Error: No .txt file found in $(Pipeline.Workspace)/pythondependencies"
         exit 1
       fi
       echo "Uploading requirements.txt to DBFS..."
       databricks fs cp "$PYTHON_PACKAGE" dbfs:/FileStore/dependencies/requirements.txt --overwrite
       echo "Successfully uploaded "$PYTHON_PACKAGE" to dbfs:/FileStore/dependencies/requirements.txt"
      displayName: 'Upload Python (requirements.txt) packages to DBFS'
    
    - script: |
        echo "Installing Python packages on cluster ID: $(databricksClusterId)"
        databricks fs cp dbfs:/FileStore/dependencies/requirements.txt requirements.txt
        pip install -r requirements.txt
        echo "Succesfully installed Python packages!"
      displayName: 'Install Python (requirements.txt) packages'

    - script: |
       echo "Backup Installing Kafka and Eventhub"
       databricks libraries install --cluster-id $(databricksClusterId) --pypi-package azure-eventhub
       databricks libraries install --cluster-id $(databricksClusterId) --pypi-package confluent-kafka
      displayName: 'Install Kafka and Eventhub to Cluster'
      
  - job: DeployHTMLtoADLS
    displayName: 'Deploy HTML to ADLS'
    pool:
      vmImage: 'ubuntu-latest'
    steps:     
      - task: AzureCLI@2
        inputs:
          azureSubscription: 'DTS-DATAINGEST-SBOX'
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            export AZURE_STORAGE_YES_PROMPT=true

            echo "Assigning Service Principal Blob Contributor Access"
            az role assignment create \
            --assignee $(Client_ID) \
            --role "Storage Blob Data Contributor" \
            --scope /subscriptions/$(Subscription_ID)/resourceGroups/$(Resource_Group)/providers/Microsoft.Storage/storageAccounts/$(HTML_StorageAccount)
            echo "Success! Service Principal has been assigned Storage Blob Data Contributor for the following scope: /subscriptions/$(Subscription_ID)/resourceGroups/$(Resource_Group)/providers/Microsoft.Storage/storageAccounts/$(HTML_StorageAccount)"

            echo "Update $(HTML_StorageAccount) to enable storage account access"
            az storage account update \
            --name $(HTML_StorageAccount) \
            --resource-group $(Resource_Group) \
            --default-action Allow
            echo "Success! $(HTML_StorageAccount) access granted!"

            echo "Output  $(HTML_StorageAccount) storage account key"
            storage_account_key=$(az storage account keys list \
            --resource-group $(Resource_Group) \
            --account-name $(HTML_StorageAccount) \
            --query '[0].value' -o tsv)
            echo "Success! $(HTML_StorageAccount) is $storage_account_key"

            echo "Retrieve  $(HTML_StorageAccount) blob endpoint"
            blob_endpoint=$(az storage account show \
            --resource-group $(Resource_Group) \
            --name $(HTML_StorageAccount) \
            --query "primaryEndpoints.blob" -o tsv)
            echo "Success! $(HTML_StorageAccount) is $blob_endpoint"
        
            echo "Create storage container html-template if it does not exist"
            --account-name $(HTML_StorageAccount) \
            --name html-template \
            --account-key $storage_account_key \
            --query exists -o tsv | grep -q 'true' || \
            az storage container create \
            --account-name $(HTML_StorageAccount) \
            --name html-template \
            --account-key $storage_account_key \
            --blob-endpoint $blob_endpoint 
            echo "Success! html-template container created!"

            echo "Uploading HTML templates to blob storage $(HTML_StorageAccount)/html-template"
            az storage blob upload-batch \
            --account-name $(HTML_StorageAccount) \
            --account-key $storage_account_key \
            --destination html-template \
            --source '$(Build.SourcesDirectory)/01-Sandbox/HTML_Templates/' \
            --pattern '*.html'
            echo "Success! HTML templates imported to $(HTML_StorageAccount)/html-template"

            echo "Update $(HTML_StorageAccount) to disable storage account access"
            az storage account update \
            --name $(HTML_StorageAccount) \
            --resource-group $(Resource_Group) \
            --default-action Deny
            echo "Success! $(HTML_StorageAccount) access granted!"

        displayName: 'Storage account and storage container configuration'